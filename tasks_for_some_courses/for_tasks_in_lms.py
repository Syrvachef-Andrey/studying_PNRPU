import torch
from torch import nn, optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)

trainloader = DataLoader(trainset, batch_size=64, shuffle=True)
testloader = DataLoader(testset, batch_size=64, shuffle=False)

# === –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• ===

# –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è Fashion-MNIST
class_names = [
    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'
]


def show_dataset_info(trainset, testset):
    """–í—ã–≤–æ–¥–∏—Ç –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    print("üìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–¢–ê–°–ï–¢–ï:")
    print(f"–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(trainset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(testset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {trainset[0][0].shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(class_names)}")
    print("–ö–ª–∞—Å—Å—ã:", class_names)
    print("-" * 50)


def show_sample_images(dataset, class_names, num_images=12):
    """
    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
    """
    # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
    indices = np.random.choice(len(dataset), num_images, replace=False)

    # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∫—É –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    fig, axes = plt.subplots(3, 4, figsize=(15, 12))
    axes = axes.ravel()

    for i, idx in enumerate(indices):
        # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –º–µ—Ç–∫—É
        image, label = dataset[idx]

        # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image = image * 0.5 + 0.5  # –æ–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        image = image.squeeze()  # —É–±–∏—Ä–∞–µ–º dimension –∫–∞–Ω–∞–ª–∞ (1,28,28) -> (28,28)

        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        axes[i].imshow(image, cmap='gray')
        axes[i].set_title(f'{label}: {class_names[label]}', fontsize=12, pad=10)
        axes[i].axis('off')

    plt.suptitle('–°–ª—É—á–∞–π–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ Fashion-MNIST', fontsize=16, y=0.95)
    plt.tight_layout()
    plt.show()


def show_batch_images(dataloader, class_names, num_images=8):
    """
    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ DataLoader
    """
    # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á
    dataiter = iter(dataloader)
    images, labels = next(dataiter)

    print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {images.shape}")  # [batch_size, channels, height, width]
    print(f"–î–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π –ø–∏–∫—Å–µ–ª–µ–π: [{images.min():.3f}, {images.max():.3f}]")

    # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∫—É –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    fig, axes = plt.subplots(2, 4, figsize=(15, 8))
    axes = axes.ravel()

    for i in range(min(num_images, len(images))):
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img = images[i]
        img = img * 0.5 + 0.5  # –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        img = img.squeeze()  # —É–±–∏—Ä–∞–µ–º dimension –∫–∞–Ω–∞–ª–∞

        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º
        axes[i].imshow(img, cmap='gray')
        axes[i].set_title(f'–ú–µ—Ç–∫–∞: {labels[i].item()} - {class_names[labels[i]]}',
                          fontsize=12, pad=10)
        axes[i].axis('off')

    plt.suptitle(f'–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ (batch_size={len(images)})',
                 fontsize=16, y=0.95)
    plt.tight_layout()
    plt.show()


def show_class_distribution(dataset, class_names):
    """
    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
    """
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–µ—Ç–∫–∏
    labels = [dataset[i][1] for i in range(len(dataset))]

    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    unique, counts = np.unique(labels, return_counts=True)

    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    plt.figure(figsize=(12, 6))
    bars = plt.bar(unique, counts, color='skyblue', edgecolor='black', alpha=0.7)

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏
    plt.xlabel('–ö–ª–∞—Å—Å—ã')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ', fontsize=16)
    plt.xticks(unique, [class_names[i] for i in unique], rotation=45, ha='right')

    # –î–æ–±–∞–≤–ª—è–µ–º —á–∏—Å–ª–∞ –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, count in zip(bars, counts):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 10,
                 f'{count}', ha='center', va='bottom', fontweight='bold')

    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.show()

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ö–õ–ê–°–°–û–í:")
    for i, (cls, count) in enumerate(zip(unique, counts)):
        percentage = (count / len(dataset)) * 100
        print(f"{class_names[cls]:15}: {count:5} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ({percentage:5.1f}%)")


def show_image_statistics(dataset):
    """
    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∏–∫—Å–µ–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    """
    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–∏–∫—Å–µ–ª—è–º
    pixels = []
    for i in range(min(1000, len(dataset))):  # –±–µ—Ä–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        image, _ = dataset[i]
        pixels.extend(image.flatten().numpy())

    pixels = np.array(pixels)

    # –°–æ–∑–¥–∞–µ–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.hist(pixels, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')
    plt.xlabel('–ó–Ω–∞—á–µ–Ω–∏–µ –ø–∏–∫—Å–µ–ª—è')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø–∏–∫—Å–µ–ª–µ–π')
    plt.grid(alpha=0.3)

    plt.subplot(1, 2, 2)
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∏–ª—è—Ö
    fig, axes = plt.subplots(2, 3, figsize=(10, 7))
    styles = ['gray', 'viridis', 'plasma', 'inferno', 'magma', 'cividis']

    for i, style in enumerate(styles):
        img, label = dataset[i]
        img = img.squeeze()
        row, col = i // 3, i % 3
        axes[row, col].imshow(img, cmap=style)
        axes[row, col].set_title(f'cmap: {style}')
        axes[row, col].axis('off')

    plt.suptitle('–†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–æ–≤—ã–µ —Å—Ö–µ–º—ã –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è', fontsize=14)
    plt.tight_layout()
    plt.show()

    print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ò–ö–°–ï–õ–ï–ô:")
    print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {pixels.min():.3f}")
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {pixels.max():.3f}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {pixels.mean():.3f}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {pixels.std():.3f}")


class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28 * 28, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits


device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

model = NeuralNetwork().to(device)
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)


# === –≠–¢–ê–ü 3: –¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø ===

def train_one_epoch(model, trainloader, loss_fn, optimizer, device, print_every=200):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ

    Args:
        model: –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
        trainloader: DataLoader —Å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        loss_fn: —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        optimizer: –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cpu/cuda)
        print_every: —á–∞—Å—Ç–æ—Ç–∞ –≤—ã–≤–æ–¥–∞ –ª–æ–≥–æ–≤ (–≤ –±–∞—Ç—á–∞—Ö)
    """
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
    model.train()

    # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    running_loss = 0.0
    total_batches = len(trainloader)

    print(f"–ù–∞—á–∞–ª–æ —ç–ø–æ—Ö–∏. –í—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {total_batches}")
    print("-" * 50)

    # –¶–∏–∫–ª –ø–æ –≤—Å–µ–º –±–∞—Ç—á–∞–º –≤ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä–µ
    for batch_idx, (data, targets) in enumerate(trainloader):
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU/CPU)
        data = data.to(device)
        targets = targets.to(device)

        # 1. –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        optimizer.zero_grad()

        # 2. –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (forward pass): –≤—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        outputs = model(data)

        # 3. –í—ã—á–∏—Å–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
        loss = loss_fn(outputs, targets)

        # 4. –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ (backward pass): –≤—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        loss.backward()

        # 5. –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞: –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
        optimizer.step()

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        running_loss += loss.item()

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ print_every –±–∞—Ç—á–µ–π
        if (batch_idx + 1) % print_every == 0:
            avg_loss = running_loss / print_every
            current_batch = batch_idx + 1
            progress = (current_batch / total_batches) * 100

            print(f'–ë–∞—Ç—á [{current_batch:4d}/{total_batches}], '
                  f'–ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:5.1f}%, '
                  f'–°—Ä–µ–¥–Ω–∏–µ –ø–æ—Ç–µ—Ä–∏: {avg_loss:.4f}')

            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º running_loss –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
            running_loss = 0.0

    # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —ç–ø–æ—Ö–∏
    if total_batches % print_every != 0:
        remaining_batches = total_batches % print_every
        if remaining_batches > 0:
            avg_loss = running_loss / remaining_batches
            print(f'–§–∏–Ω–∞–ª—å–Ω—ã–µ [{total_batches:4d}/{total_batches}], '
                  f'–°—Ä–µ–¥–Ω–∏–µ –ø–æ—Ç–µ—Ä–∏: {avg_loss:.4f}')


# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ
def calculate_accuracy(model, dataloader, device):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞–Ω–Ω–æ–º DataLoader
    """
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for data, targets in dataloader:
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()

    accuracy = 100 * correct / total
    return accuracy


# === –ó–ê–ü–£–°–ö –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ò –û–ë–£–ß–ï–ù–ò–Ø ===
if __name__ == "__main__":
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    print("üîç –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–•")
    print("=" * 60)

    # 1. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
    show_dataset_info(trainset, testset)

    # 2. –°–ª—É—á–∞–π–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞
    print("\nüñºÔ∏è –°–õ–£–ß–ê–ô–ù–´–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø –ò–ó –¢–†–ï–ù–ò–†–û–í–û–ß–ù–û–ì–û –ù–ê–ë–û–†–ê:")
    show_sample_images(trainset, class_names)

    # 3. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞
    print("\nüì¶ –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø –ò–ó –ü–ï–†–í–û–ì–û –ë–ê–¢–ß–ê:")
    show_batch_images(trainloader, class_names)

    # 4. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    print("\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í –í –¢–†–ï–ù–ò–†–û–í–û–ß–ù–û–ú –ù–ê–ë–û–†–ï:")
    show_class_distribution(trainset, class_names)

    # 5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∏–∫—Å–µ–ª–µ–π
    print("\nüé® –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô:")
    show_image_statistics(trainset)

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n" + "=" * 60)
    print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò")
    print("=" * 60)

    # –û–±—É—á–∞–µ–º –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ
    train_one_epoch(model, trainloader, loss_fn, optimizer, device, print_every=100)

    # –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ —ç–ø–æ—Ö–∏
    train_accuracy = calculate_accuracy(model, trainloader, device)
    print(f"\n‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ –ø–æ—Å–ª–µ —ç–ø–æ—Ö–∏: {train_accuracy:.2f}%")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
    torch.save(model.state_dict(), 'model_after_one_epoch.pth')
    print("üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'model_after_one_epoch.pth'")